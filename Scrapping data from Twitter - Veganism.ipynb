{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8791d6a7",
   "metadata": {},
   "source": [
    "# 1. Scrapping data from twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6773d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q snscrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd1987d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "202d0bae",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping, 100 results so far\n",
      "Scraping, 200 results so far\n",
      "Scraping, 300 results so far\n",
      "Scraping, 400 results so far\n",
      "Scraping, 500 results so far\n",
      "Scraping, 600 results so far\n",
      "Scraping, 700 results so far\n",
      "Scraping, 800 results so far\n",
      "Scraping, 900 results so far\n",
      "Scraping, 1000 results so far\n",
      "Scraping, 1100 results so far\n",
      "Scraping, 1200 results so far\n",
      "Scraping, 1300 results so far\n",
      "Scraping, 1400 results so far\n",
      "Scraping, 1500 results so far\n",
      "Scraping, 1600 results so far\n",
      "Scraping, 1700 results so far\n",
      "Scraping, 1800 results so far\n",
      "Scraping, 1900 results so far\n",
      "Scraping, 2000 results so far\n",
      "Stopped scraping after 2000 results due to --max-results\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(\"snscrape --jsonl --progress --max-results 2000 --since 2018-01-01 twitter-hashtag 'veganism until:2022-10-01' > veganism-tweets2022.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36af800",
   "metadata": {},
   "source": [
    "1. max-results: how many tweets you want\n",
    "2. since: the starting date\n",
    "3. twitter-hashtag: to look for a specific topic\n",
    "4. until: the end date\n",
    "5. \\> to save the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6f67e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping, 100 results so far\n",
      "Scraping, 200 results so far\n",
      "Scraping, 300 results so far\n",
      "Scraping, 400 results so far\n",
      "Scraping, 500 results so far\n",
      "Scraping, 600 results so far\n",
      "Scraping, 700 results so far\n",
      "Scraping, 800 results so far\n",
      "Scraping, 900 results so far\n",
      "Scraping, 1000 results so far\n",
      "Scraping, 1100 results so far\n",
      "Scraping, 1200 results so far\n",
      "Scraping, 1300 results so far\n",
      "Scraping, 1400 results so far\n",
      "Scraping, 1500 results so far\n",
      "Scraping, 1600 results so far\n",
      "Scraping, 1700 results so far\n",
      "Scraping, 1800 results so far\n",
      "Scraping, 1900 results so far\n",
      "Scraping, 2000 results so far\n",
      "Stopped scraping after 2000 results due to --max-results\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(\"snscrape --jsonl --progress --max-results 2000 --since 2018-01-01 twitter-hashtag 'veganism until:2021-10-01' > veganism-tweets2021.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c70bdccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping, 100 results so far\n",
      "Scraping, 200 results so far\n",
      "Scraping, 300 results so far\n",
      "Scraping, 400 results so far\n",
      "Scraping, 500 results so far\n",
      "Scraping, 600 results so far\n",
      "Scraping, 700 results so far\n",
      "Scraping, 800 results so far\n",
      "Scraping, 900 results so far\n",
      "Scraping, 1000 results so far\n",
      "Scraping, 1100 results so far\n",
      "Scraping, 1200 results so far\n",
      "Scraping, 1300 results so far\n",
      "Scraping, 1400 results so far\n",
      "Scraping, 1500 results so far\n",
      "Scraping, 1600 results so far\n",
      "Scraping, 1700 results so far\n",
      "Scraping, 1800 results so far\n",
      "Scraping, 1900 results so far\n",
      "Scraping, 2000 results so far\n",
      "Stopped scraping after 2000 results due to --max-results\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(\"snscrape --jsonl --progress --max-results 2000 --since 2018-01-01 twitter-hashtag 'veganism until:2020-10-01' > veganism-tweets2020.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a9c5937",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping, 100 results so far\n",
      "Scraping, 200 results so far\n",
      "Scraping, 300 results so far\n",
      "Scraping, 400 results so far\n",
      "Scraping, 500 results so far\n",
      "Scraping, 600 results so far\n",
      "Scraping, 700 results so far\n",
      "Scraping, 800 results so far\n",
      "Scraping, 900 results so far\n",
      "Scraping, 1000 results so far\n",
      "Scraping, 1100 results so far\n",
      "Scraping, 1200 results so far\n",
      "Scraping, 1300 results so far\n",
      "Scraping, 1400 results so far\n",
      "Scraping, 1500 results so far\n",
      "Scraping, 1600 results so far\n",
      "Scraping, 1700 results so far\n",
      "Scraping, 1800 results so far\n",
      "Scraping, 1900 results so far\n",
      "Scraping, 2000 results so far\n",
      "Stopped scraping after 2000 results due to --max-results\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(\"snscrape --jsonl --progress --max-results 2000 --since 2018-01-01 twitter-hashtag 'veganism until:2019-10-01' > veganism-tweets2019.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf0507fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping, 100 results so far\n",
      "Scraping, 200 results so far\n",
      "Scraping, 300 results so far\n",
      "Scraping, 400 results so far\n",
      "Scraping, 500 results so far\n",
      "Scraping, 600 results so far\n",
      "Scraping, 700 results so far\n",
      "Scraping, 800 results so far\n",
      "Scraping, 900 results so far\n",
      "Scraping, 1000 results so far\n",
      "Scraping, 1100 results so far\n",
      "Scraping, 1200 results so far\n",
      "Scraping, 1300 results so far\n",
      "Scraping, 1400 results so far\n",
      "Scraping, 1500 results so far\n",
      "Scraping, 1600 results so far\n",
      "Scraping, 1700 results so far\n",
      "Scraping, 1800 results so far\n",
      "Scraping, 1900 results so far\n",
      "Scraping, 2000 results so far\n",
      "Stopped scraping after 2000 results due to --max-results\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(\"snscrape --jsonl --progress --max-results 2000 --since 2018-01-01 twitter-hashtag 'veganism until:2018-10-01' > veganism-tweets2018.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aad6c72",
   "metadata": {},
   "source": [
    "### Reading the json generated from the CLI commands above and creating pandas dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2214201",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df2022 = pd.read_json('veganism-tweets2022.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4751a74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df2021 = pd.read_json('veganism-tweets2021.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4fd63b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df2020 = pd.read_json('veganism-tweets2020.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbb794cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df2019 = pd.read_json('veganism-tweets2019.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72b69885",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df2018 = pd.read_json('veganism-tweets2018.json', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975f481e",
   "metadata": {},
   "source": [
    "### Saving all the dataframes (converting json files into csv files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ccfd5b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df2022.to_csv('veganism_tweets2022.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63b1486c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df2021.to_csv('veganism_tweets2021.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22bdc996",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df2020.to_csv('veganism_tweets2020.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "457606c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df2019.to_csv('veganism_tweets2019.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e711524f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df2018.to_csv('veganism_tweets2018.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b966d8c5",
   "metadata": {},
   "source": [
    "### Merging all the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42a3211c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(map(pd.read_csv, ['veganism_tweets2022.csv', 'veganism_tweets2021.csv', 'veganism_tweets2020.csv', 'veganism_tweets2019.csv', 'veganism_tweets2018.csv' ]), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9cd5271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 29 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   Unnamed: 0        10000 non-null  int64  \n",
      " 1   _type             10000 non-null  object \n",
      " 2   url               10000 non-null  object \n",
      " 3   date              10000 non-null  object \n",
      " 4   content           10000 non-null  object \n",
      " 5   renderedContent   10000 non-null  object \n",
      " 6   id                10000 non-null  int64  \n",
      " 7   user              10000 non-null  object \n",
      " 8   replyCount        10000 non-null  int64  \n",
      " 9   retweetCount      10000 non-null  int64  \n",
      " 10  likeCount         10000 non-null  int64  \n",
      " 11  quoteCount        10000 non-null  int64  \n",
      " 12  conversationId    10000 non-null  int64  \n",
      " 13  lang              10000 non-null  object \n",
      " 14  source            10000 non-null  object \n",
      " 15  sourceUrl         10000 non-null  object \n",
      " 16  sourceLabel       10000 non-null  object \n",
      " 17  outlinks          5790 non-null   object \n",
      " 18  tcooutlinks       5790 non-null   object \n",
      " 19  media             4560 non-null   object \n",
      " 20  retweetedTweet    0 non-null      float64\n",
      " 21  quotedTweet       699 non-null    object \n",
      " 22  inReplyToTweetId  694 non-null    float64\n",
      " 23  inReplyToUser     694 non-null    object \n",
      " 24  mentionedUsers    1853 non-null   object \n",
      " 25  coordinates       372 non-null    object \n",
      " 26  place             373 non-null    object \n",
      " 27  hashtags          9806 non-null   object \n",
      " 28  cashtags          8 non-null      object \n",
      "dtypes: float64(2), int64(7), object(20)\n",
      "memory usage: 2.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a7a499",
   "metadata": {},
   "source": [
    "### Exporting the final dataframe with tweets from 2018 to 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57f24a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('final_veganism_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f4d639",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
